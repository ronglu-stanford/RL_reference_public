{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Deep Learning for NLP (ICME Summer Workshop, 2021)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronglu-stanford/RL_reference_public/blob/main/Deep_Learning_for_NLP_(ICME_Summer_Workshop%2C_2021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3XJ0X7HHSo4"
      },
      "source": [
        "# Deep Learning for Natural Language Processing\n",
        "\n",
        "*Instructor: Sam Gorman*\n",
        "\n",
        "*Created By: Luke de Oliveira*\n",
        "\n",
        "*Date: July 21, 2021*\n",
        "\n",
        "## Structure\n",
        "\n",
        "This notebook is split up into three parts. \n",
        "\n",
        "The first part is an introduction to \"wrangling\" text data in Python and how to prepare text data for use with Machine Learning algorithms. \n",
        "\n",
        "The second part walks through an implementation of a sentiment detection model with a Long Short-Term Memory network (LSTM). \n",
        "\n",
        "The third part will use a demo dataset to train a Semantic Chunking model for a conversational agent. \n",
        "\n",
        "To use a hardware accelerator (i.e., a GPU) navigate in the menu above to **`Runtime > Change runtime type > GPU`**.\n",
        "\n",
        "## License\n",
        "\n",
        "All code examples and code downloads are licensed under the (extremely permissive) [MIT license](https://opensource.org/licenses/MIT). My goal is to have this be a useful base for you, should you so desire.\n",
        "\n",
        "## Datasets\n",
        "\n",
        "This notebook will use two datasets: \n",
        "\n",
        "* A binary sentiment dataset, with reviews / produced content from Yelp, Amazon, and Twitter\n",
        "* A semantic chunking dataset for a virtual assistant use case, where we'll be understanding weather queries\n",
        "\n",
        "To download the sentiment dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SxHIRSIyOu"
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/sentiment-data.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wz8Sev1I1qP"
      },
      "source": [
        "To download the virtual assistant dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0vrZjG-I2jE"
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/weather-assistant.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdzT82cme1s"
      },
      "source": [
        "To download the `icmenlp` package, we run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chc-IqyCme1s"
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/icmenlp.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vylb_7kNMhVO"
      },
      "source": [
        "Let's take a look in our VM's directory..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "royXp7wZnGuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e026d5a7-8e36-4c68-fd22-f7f348779b7d"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "icmenlp.py  \u001b[0m\u001b[01;34msample_data\u001b[0m/  sentiment-data.json  weather-assistant.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7a7cqL2NGYA"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll be using Colaboratory built-in libraries (scikit-learn & Keras) in order to avoid set up!\n",
        "\n",
        "Let's set up our imports below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d0r_nXSme10"
      },
      "source": [
        "# Make sure if we change any imports, they're reflected in our notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PNT-R4TM-jN"
      },
      "source": [
        "from tensorflow import keras\n",
        "#Necessary in order to import sequence module \n",
        "#https://github.com/keras-team/keras/issues/8715\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTKdzjdXme15"
      },
      "source": [
        "Now, we'll use our library for this tutorial - `icmenlp`. This library provides two main utilities -- first, a principled way to load the data for this session, and second, a vocabulary container, which we will describe later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb3rbWtNme16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fada9497-3af0-4217-c213-5ed0a631f8b9"
      },
      "source": [
        "import icmenlp\n",
        "!cat icmenlp.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\"\"\n",
            "file: icmenlp.py\n",
            "description: Utilities for the ICME summer workshop on Deep Learning for NLP\n",
            "copyright: 2018 Luke de Oliveira, MIT Licensed.\n",
            "date: August 17th, 2018\n",
            "\"\"\"\n",
            "\n",
            "import json\n",
            "\n",
            "import keras\n",
            "import numpy as np\n",
            "from sklearn.base import TransformerMixin\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "\n",
            "def load_data(path, source='assistant', splits=(70, 10, 20)):\n",
            "    \"\"\"\n",
            "    Loads data type specified from the file passed in.\n",
            "\n",
            "    Args:\n",
            "        path (Path): A path to a file (json) to load from.\n",
            "        source (str): One of 'assistant' or 'sentiment'\n",
            "        splits (tuple): A length-3 tuple containing the training split size,\n",
            "            val split size, and testing split size respectively.\n",
            "\n",
            "    Returns:\n",
            "        Dict: A dictionary with keys 'train', 'val', and 'test', holding each\n",
            "            respective set.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If you do not pass in a tuple of length three into splits.\n",
            "    \"\"\"\n",
            "    if not len(splits) == 3:\n",
            "        raise ValueError('splits expected to have three components: found {}'\n",
            "                         .format(len(splits)))\n",
            "    train_size, test_size, val_size = np.array(splits) / np.sum(splits)\n",
            "\n",
            "    data = json.load(open(path))\n",
            "    if source == 'assistant':\n",
            "        # Split into training set & (test + val) set.\n",
            "        data_train, data_test = train_test_split(\n",
            "            data, train_size=train_size\n",
            "        )\n",
            "        # Now, the (test + val) set into the test, and val sets\n",
            "        data_test, data_val = train_test_split(\n",
            "            data_test,\n",
            "            test_size=(val_size / (val_size + test_size))\n",
            "        )\n",
            "        data = {\n",
            "            'train': data_train,\n",
            "            'test': data_test,\n",
            "            'val': data_val\n",
            "        }\n",
            "    elif source == 'sentiment':\n",
            "        text, label = data['text'], data['label']\n",
            "\n",
            "        # Split into training set & (test + val) set.\n",
            "        text_train, text_test, label_train, label_test = train_test_split(\n",
            "            text, label,\n",
            "            train_size=train_size\n",
            "        )\n",
            "        # Now, the (test + val) set into the test, and val sets\n",
            "        text_test, text_val, label_test, label_val = train_test_split(\n",
            "            text_test, label_test,\n",
            "            test_size=(val_size / (val_size + test_size))\n",
            "        )\n",
            "        data = {\n",
            "            'train': (text_train, label_train),\n",
            "            'test': (text_test, label_test),\n",
            "            'val': (text_val, label_val)\n",
            "        }\n",
            "    else:\n",
            "        raise ValueError('Invalid source: {}'.format(source))\n",
            "    return data\n",
            "\n",
            "\n",
            "class VocabularyContainer(TransformerMixin):\n",
            "\n",
            "    PAD_TOKEN = '<PAD>'\n",
            "    UNK_TOKEN = '<UNK>'\n",
            "    START_TOKEN = '<S>'\n",
            "    END_TOKEN = '</S>'\n",
            "\n",
            "    def __init__(self, tokenizer=None):\n",
            "        \"\"\"Create a new VocabularyContainer object that will hold a stateful,\n",
            "        fittable vocabulary.\n",
            "\n",
            "        Args:\n",
            "            tokenizer (Callable): A function that maps an object into a list of\n",
            "                strings. By default, this will split tokens based on spaces.\n",
            "        \"\"\"\n",
            "        self.tokenizer = tokenizer or (lambda x: x.split(' '))\n",
            "        self._word2index = {\n",
            "            self.PAD_TOKEN: 0,\n",
            "            self.UNK_TOKEN: 1,\n",
            "            self.START_TOKEN: 2,\n",
            "            self.END_TOKEN: 3,\n",
            "        }\n",
            "        self._index2word = {}\n",
            "\n",
            "    def fit(self, documents):\n",
            "        \"\"\"Fits the vocab on a set of documents.\n",
            "\n",
            "        Will apply the tokenizer specified in the constructor to each document\n",
            "        to extract the tokens.\n",
            "\n",
            "        Args:\n",
            "            documents (List[str]): Documents to tokenize and then use as the\n",
            "                vocabulary.\n",
            "\n",
            "        Returns:\n",
            "            self\n",
            "        \"\"\"\n",
            "        # Get a list of all the unique tokens that appear\n",
            "        vocab = list({\n",
            "            token for doc in documents\n",
            "            for token in self.tokenizer(doc)\n",
            "            if token not in self._word2index\n",
            "        })\n",
            "\n",
            "        # This is UNK, START, END, and PAD.\n",
            "        nb_special_tokens = 4\n",
            "\n",
            "        # First, we map token -> ID, leaving the first slots for special tokens\n",
            "        self._word2index.update({\n",
            "            word: idx\n",
            "            for idx, word in enumerate(vocab, nb_special_tokens)\n",
            "        })\n",
            "\n",
            "        # Next, we invert this map, which we can do since it was built from\n",
            "        # unique vocabulary elements and is by definition bijective.\n",
            "        self._index2word.update({\n",
            "            idx: word\n",
            "            for word, idx in self._word2index.items()\n",
            "        })\n",
            "\n",
            "        return self\n",
            "\n",
            "    @property\n",
            "    def vocab_size(self):\n",
            "        return len(self._word2index)\n",
            "\n",
            "    def _to_indices(self, obj):\n",
            "        # Recursively converts iterables of strings to iterables of integers\n",
            "        if isinstance(obj, str):\n",
            "            return self._word2index.get(obj, self._word2index[self.UNK_TOKEN])\n",
            "        return [self._to_indices(o) for o in obj]\n",
            "\n",
            "    def _to_words(self, obj):\n",
            "        # Recursively converts iterables of integers to iterables of strings\n",
            "        if isinstance(obj, int):\n",
            "            return self._index2word.get(obj, self.UNK_TOKEN)\n",
            "        return [self._to_words(o) for o in obj]\n",
            "\n",
            "    def process_text(self, text, add_start=False, add_end=False):\n",
            "        \"\"\"Converts a single document/text into a integer representation.\n",
            "\n",
            "        Args:\n",
            "            text (str): Document to convert to it's integer representation.\n",
            "            add_start (bool): Whether or not to add the START token <S> to the\n",
            "                document or not.\n",
            "            add_end (bool): Whether or not to add the END token </S> to the\n",
            "                document or not.\n",
            "\n",
            "        Returns:\n",
            "            List[int]: integer representations for each token in the tokenized\n",
            "                version of the passed in text.\n",
            "        \"\"\"\n",
            "        text = self.tokenizer(text)\n",
            "        indices = self._to_indices(text)\n",
            "\n",
            "        if add_start:\n",
            "            indices = [self._to_indices(self.START_TOKEN)] + indices\n",
            "\n",
            "        if add_end:\n",
            "            indices = indices + [self._to_indices(self.END_TOKEN)]\n",
            "\n",
            "        return indices\n",
            "\n",
            "    def transform(self, documents, pad_length=None, add_start=False,\n",
            "                  add_end=False):\n",
            "        \"\"\"Transforms a series of documents into a list of equal length lists\n",
            "        of integers (each sublist is a tokenized document in integer\n",
            "        representation form).\n",
            "\n",
            "        Args:\n",
            "            documents (List[str]): List of documents to transform.\n",
            "            pad_length (int): Length to pad documents to.\n",
            "            add_start (bool): Whether or not to add the START token <S> to the\n",
            "                document or not.\n",
            "            add_end (bool): Whether or not to add the END token </S> to the\n",
            "                document or not.\n",
            "\n",
            "        Returns:\n",
            "            List[List[int]]: integer representations for each token in the\n",
            "                tokenized version of the passed in text.\n",
            "        \"\"\"\n",
            "        indices = [\n",
            "            self.process_text(text=doc, add_start=add_start, add_end=add_end)\n",
            "            for doc in documents\n",
            "        ]\n",
            "        if pad_length:\n",
            "            if pad_length == 'max':\n",
            "                pad_length = max(map(len, indices))\n",
            "            indices = keras.preprocessing.sequence.pad_sequences(\n",
            "                sequences=indices,\n",
            "                maxlen=pad_length,\n",
            "                dtype=int,\n",
            "                padding='pre',\n",
            "                truncating='pre'\n",
            "            ).tolist()\n",
            "        return indices\n",
            "\n",
            "    def inverse_transform(self, indices):\n",
            "        \"\"\"Takes a sequence of integers and maps back into tokens.\"\"\"\n",
            "        return self._to_words(obj=indices)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Je6C42xMv_q"
      },
      "source": [
        "Let's open our datasets using data loading functions that will provide us with a train-test-validate split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tnt_cRTOWU2"
      },
      "source": [
        "ASSISTANT_DATA = 'weather-assistant.json'\n",
        "SENTIMENT_DATA = 'sentiment-data.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvoK8vprM75p"
      },
      "source": [
        "assistant_data = icmenlp.load_data(ASSISTANT_DATA, 'assistant')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-SOpTIOC-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff65910-1d36-4d2b-98b2-a848c80ba3a7"
      },
      "source": [
        "assistant_data['train'][4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Is it going to be '},\n",
              " {'label': 'condition_description', 'text': 'windy'},\n",
              " {'text': ' '},\n",
              " {'label': 'current_location', 'text': 'here'},\n",
              " {'text': ' on '},\n",
              " {'label': 'timeRange', 'text': 'Nov. 17'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIlx7AGme2E"
      },
      "source": [
        "# Define a small utility to display chunked data a bit more easily\n",
        "def display_chunking(chunks):\n",
        "    sent = ''\n",
        "    for ch in chunks:\n",
        "        label = ch.get('label')\n",
        "        text = ch['text']\n",
        "        if not label:\n",
        "            sent += text\n",
        "        else:\n",
        "            sent += '[{} | {}]'.format(text, label.upper())\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgzNOUhsme2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebd609a7-9aad-4e8f-ec2c-69aa3db21209"
      },
      "source": [
        "display_chunking(assistant_data['train'][4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Is it going to be [windy | CONDITION_DESCRIPTION] [here | CURRENT_LOCATION] on [Nov. 17 | TIMERANGE]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfbEaCIzGJPK"
      },
      "source": [
        "## Data Exploration and Visualization\n",
        "You'll save yourself from future headaches in more complex deep learning specific tasks later in the pipeline by taking the time to understand your data. Like classical ML methods, deep learning models can only perform to the level of quality of data you provide.  Poor labeling, statistical bias, overlap in train / val / test sets and more can negatively affect performance of deep learning models. Here, we provide some cursory scaffolding for data exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwqBqF-4TX-B"
      },
      "source": [
        "[Pandas](https://pandas.pydata.org/docs/reference/index.html#api) is a popular library that provides high-level building blocks for data in Python with the notion of a dataframe object. In practice in deep learning, we use tensors with libraries such as Tensorflow or Pytorch, but rapid data exploration in Pandas remains a helpful preliminary step before proceeding further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vYF2C0aGLXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36830007-a8ef-4e08-9562-bb3b68c8b712"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "sentiment_data = icmenlp.load_data(SENTIMENT_DATA, 'sentiment')\n",
        "\n",
        "train_df = pd.DataFrame.from_dict(sentiment_data['train'])\n",
        "train_df = train_df.transpose()\n",
        "train_df.columns = ['text','label']\n",
        "\n",
        "test_df = pd.DataFrame.from_dict(sentiment_data['test'])\n",
        "test_df = test_df.transpose()\n",
        "test_df.columns = ['text','label']\n",
        "\n",
        "\n",
        "test_df.info() #Provides a useful summary of the dataframe\n",
        "#Note there are several other helpful Pandas methods for data exploration\n",
        "#test_df.head() #This method provides a snapshot of the top n elements in the dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 300 entries, 0 to 299\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    300 non-null    object\n",
            " 1   label   300 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 4.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "guW386gHKusW",
        "outputId": "c515cc2b-be1e-4fb4-edd9-022598a1e482"
      },
      "source": [
        "#Helper function to randomly sample 10 text,label pairs \n",
        "def display_positives_negatives(train_or_test_dict):\n",
        "   sample = train_or_test_dict.sample(10)\n",
        "   return sample\n",
        "\n",
        "\n",
        "display(display_positives_negatives(test_df))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>i saw it as a child on tv back in 1973, when i...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>the food was outstanding and the prices were v...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>it's a sad movie, but very good.</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>i cannot believe that the actors agreed to do ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>worked perfectly!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>if you don't find it, too bad, as again the un...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>very slow at seating even with reservation.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>during several different 2 minute calls, i exp...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>it lasts less than 3o minutes, if i actually t...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>yes, i am simplifying things here for the sake...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text     label\n",
              "98   i saw it as a child on tv back in 1973, when i...  positive\n",
              "68   the food was outstanding and the prices were v...  positive\n",
              "119                   it's a sad movie, but very good.  positive\n",
              "224  i cannot believe that the actors agreed to do ...  negative\n",
              "201                                  worked perfectly!  positive\n",
              "27   if you don't find it, too bad, as again the un...  negative\n",
              "184        very slow at seating even with reservation.  negative\n",
              "176  during several different 2 minute calls, i exp...  negative\n",
              "260  it lasts less than 3o minutes, if i actually t...  negative\n",
              "292  yes, i am simplifying things here for the sake...  negative"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxUBtzdR5Fy"
      },
      "source": [
        "The graphical libary [Seaborn](https://seaborn.pydata.org/) is an easy-to-use wrapper for Matplot.lib. You can easily create visualizations for viewing label distributions, most common / uncommon tokens or ngrams, discrepencies in train/test data, and more.    Here, we use Seaborn (often abbreviated as sns in code) to observe that the distribution of the labels in our test set is roughly equivalent before proceeding further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "V7fQeQSLRXUU",
        "outputId": "b79c53ea-2aca-4649-a95a-781c38ffdb13"
      },
      "source": [
        "import seaborn as sns \n",
        "g = sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7lr0deyme2J"
      },
      "source": [
        "# Manipulating Text Data for ML\n",
        "\n",
        "One of the most asked questions both from students and from industry concerns how to prepare text data for deep learning. Today, we're going to focus on **embeddings** (one of the more popular incarnations of this is Word2Vec). Right now, we'll learn how to prepare data for usage in a model that learns to embed text.\n",
        "\n",
        "The first step of any such pipeline is **tokenization**, that is, converting a single text or document into a *sequence* of *tokens*. For word level models, these tokens roughly correspond to words / contractions, and in a character model, this corresponds to individual bytes. Many modern methods use **subword** information, which allows you to make predictions over text that has words that were not trained on (the so-called OOV, or out of vocabulary, problem).\n",
        "\n",
        "For example, the sentence\n",
        "    \n",
        "    Is there a minimum balance I need to maintain in my accounts?\n",
        "    \n",
        "could be *tokenized* as:\n",
        "    \n",
        "    'Is', 'there', 'a', 'minimum', 'balance', 'I', 'need', 'to', 'maintain', 'in', 'my', 'accounts', '?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jm9Ej8Nme2K"
      },
      "source": [
        "How can one systematically convert text into these *tokens* then? It turns out this is one of the most critical,  important, and underappreciated steps. It drastically differs from language to language, and requires a lot of care to ensure consistency. This is one of the reasons why **character level** or **subword** models can be so useful in applied settings with inconsistent spelling, grammar, and nomenclature.\n",
        "\n",
        "The dominant approach to doing word-based tokenization consists of using a [**regular expression**](https://en.wikipedia.org/wiki/Regular_expression). Regular expressions define a formal language for searching through strings for matches to a query. We'll use one here to work with our text in this tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcXtECQ_me2K"
      },
      "source": [
        "# Re is the Python RegEx library\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    return [\n",
        "        # Make sure there is no trailing whitespace\n",
        "        x.strip() \n",
        "        # Split the text on matches of at least one \"word\"\n",
        "        for x in re.split('(\\W+)', text)     # match on non-alpha / non-numeric characters\n",
        "                                             # will also return the splitting chars\n",
        "        # Only include the token if it is not null\n",
        "        if x.strip() \n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZAdMwC9me2M"
      },
      "source": [
        "Let's load the sentiment dataset to try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSdhtdOme2N"
      },
      "source": [
        "sentiment_data = icmenlp.load_data(SENTIMENT_DATA, 'sentiment')\n",
        "\n",
        "# Let's grab out a random training point\n",
        "# 0 is text (1 is labels)\n",
        "# (2 is a specific element)\n",
        "text = sentiment_data['train'][0][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdGQHpaume2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd06bbc8-77f7-4194-828a-f1d00473259a"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i have 2-3 bars on my cell phone when i am home, but you cant not hear anything.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2eNvo12me2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d54038b-48cf-40eb-f512-9cebd3587880"
      },
      "source": [
        "print(tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'have', '2', '-', '3', 'bars', 'on', 'my', 'cell', 'phone', 'when', 'i', 'am', 'home', ',', 'but', 'you', 'cant', 'not', 'hear', 'anything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSo-fpMwr5xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d777c71-174b-4b22-8f36-cc6565b6f7e8"
      },
      "source": [
        "#Let's take a look at our labels!\n",
        "print(sentiment_data['train'][1][2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsyWN6VOme2U"
      },
      "source": [
        "We will get into more detail about how embedding models work later on, but for now, let's discuss the conversion of text into a format that is useful for embedding models. Embedding models rely on an **integer** representation for each word, since we will use it as lookup into a **lookup table**.\n",
        "\n",
        "An important consideration when using deep learning models is the length of a piece of text, as well as a signal of the beginning and end of a sentence. Most deep learning models require that each batch of text passed in to the model have identical sentence lengths. We commonly solve this using **padding**, or adding a series of meaningless tokens to increase the length of a document. We can then use **masking** to ensure that our model does not incorporate these into the learning procedure.\n",
        "\n",
        "We'll use `<PAD>` as the pad token, and `<S>`/`</S>` to delimit the beginning of a sentence and the end of a sentence respectively. In addition, any tokens that are unknown to us (for example, a word that is in the test set but not the train set) are mapped to the `<UNK>` token.\n",
        "\n",
        "To map from tokens to integers, we will define a bijective (two-way) map from words to integers. This mapping is generated by considering all tokens in all documents and then assigning integers (up to MAX_VOCAB). Later the model will learn a vector representations for each element of the vocab (n-dimensional vectors where each entry is a float).\n",
        "\n",
        "We're going to use the one from our `icmenlp` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCctB5Tme2V"
      },
      "source": [
        "# Let's look inside the code of the utility provxided\n",
        "#You can use the ?? to open the referenced code from another file\n",
        "icmenlp.VocabularyContainer??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5odW8dlZme2Z"
      },
      "source": [
        "# Create a vocab collection object with the tokenizer we defined above\n",
        "vocab = icmenlp.VocabularyContainer(tokenizer=tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHBFEd37me2b"
      },
      "source": [
        "text = sentiment_data['train'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iav7-YqPme2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027d109d-8d06-4305-c81b-bf504690703d"
      },
      "source": [
        "vocab.fit(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<icmenlp.VocabularyContainer at 0x7f75eeee7650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84iwuqrme2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2bf28b-e1ec-45ae-aea6-9fb2b6aa8c7d"
      },
      "source": [
        "vocab.transform(['this product was bad!'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3178, 1528, 3394, 3525, 3049]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qsbzVxhme2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a704fe9-f340-45c8-fdda-f9f461c1d4ab"
      },
      "source": [
        "#getting an err here\n",
        "\n",
        "vocab.transform(['this product was bad!', 'meh'], pad_length=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 3178, 1528, 3394, 3525, 3049],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1345]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqV7-RZtvgAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba904458-3aa0-4a78-994e-d60addfacb09"
      },
      "source": [
        "vocab.transform(['this product was bad!'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3178, 1528, 3394, 3525, 3049]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4qth8o-me2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739fb938-339c-47ef-d5d8-0691b2059753"
      },
      "source": [
        "vocab.transform(['this product was abhorrent!'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3178, 1528, 3394, 1, 3049]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdA5cAYeme2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bdd287-c1c4-415c-fa1c-cd8e40af9316"
      },
      "source": [
        "vocab.inverse_transform(vocab.transform(['this product was abhorrent!']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'product', 'was', '<UNK>', '!']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHdhbHaeme2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2c0922-395a-444b-83c2-13516a93ce69"
      },
      "source": [
        "vocab.inverse_transform(\n",
        "    vocab.transform(\n",
        "        ['this product was abhorrent!'], \n",
        "        pad_length=9, \n",
        "        add_start=True, \n",
        "        add_end=True\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<PAD>', '<PAD>', '<S>', 'this', 'product', 'was', '<UNK>', '!', '</S>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEOxZIQme2p"
      },
      "source": [
        "We now know how to preprocess text data for use in deep learning. To summarize:\n",
        "\n",
        "1.) Each document gets split into tokens, a process called tokenization.\n",
        "\n",
        "2.) A mapping is created from the vocabulary to unique integers.\n",
        "\n",
        "3.) We have four special tokens -- the pad token `<PAD>`, the start-of-sentence token `<S>`, the end-of-sentence token `</S>`and the unknown token `<UNK>`.\n",
        "\n",
        "4.) We pad sentences with the `<PAD>` token to make them the same size.\n",
        "\n",
        "Now, let's learn about deep learning for NLP!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSUSN63Qme2p"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "For the next segment, we're going to train a sentiment prediction model using deep learning. In particular, we're going to use a recurrent neural network (RNN) to process text **token by token**. We'll be using words as tokens.\n",
        "\n",
        "An RNN uses a **cell** to process each timestep of s sequence. In this case, the cell of our RNN will process text one word at a time. A common problem with RNNs is that early timesteps are forgotten, and our network has no signal related to early timesteps. A Long short-term memory (LSTM) network solves this by introducing a better method for retaining state/memory via a **memory cell**, and introducing a **forget gate**, which allows the network to learn when to forget.\n",
        "\n",
        "Let's walk through how to build a model for our sentiment analysis task. \n",
        "\n",
        "*Best-practice tip*: we're going to create and use a **new class** to make sure we can keep track of the preprocessing that goes into such a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym6kxncPme2q"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "class BinarySentimentModel():\n",
        "\n",
        "    def __init__(self, embedding_dim=128, lstm_size=256, bidirectional=False,\n",
        "                 optimizer='adam'):\n",
        "        \"\"\"Create a new model to handle a binary sentiment task.\n",
        "\n",
        "        The wrapper class will hold all state with respect to preprocessing,\n",
        "        transformation, and model training.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the \"word vectors\" to be\n",
        "                trained in the model.\n",
        "            lstm_size (int): The number of hidden units in the LSTM.\n",
        "            bidirectional (bool): Whether or not to process the data using a\n",
        "                bidirectional or single-directional LSTM.\n",
        "            optimizer (str | keras.optimizers.Optimizer): The optimizer to use\n",
        "                when optimizing the loss on the training set using Stochastic\n",
        "                Gradient Descent (SGD).\n",
        "        \"\"\"\n",
        "        # We need to convert the names of labels to integers for our model\n",
        "        self.labelencoder = LabelEncoder()\n",
        "\n",
        "        # We're going to use our vocab container from before to store all the\n",
        "        # token -> ID mappings\n",
        "        self.vocab = icmenlp.VocabularyContainer(tokenizer=tokenize)\n",
        "        self.model = None\n",
        "\n",
        "        # We're storing *hyperparameters* of the model here\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_size = lstm_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def make_model(self, vocab_size,shouldVisualize):\n",
        "        \"\"\"Creates a new keras model for the class.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of unique tokens contained in the\n",
        "                training vocabulary\n",
        "\n",
        "        Returns:\n",
        "            keras.models.Model: A built and compiled Keras model for thre task.\n",
        "        \"\"\"\n",
        "\n",
        "        #To use the Keras functional API, we define the shape of our inputs\n",
        "        # The None tells Keras that we can expect differing sentence lengths\n",
        "        text = keras.Input(shape=(None, ), dtype='int32')\n",
        "        \n",
        "        # We're going to learn word vectors. Since our sentences\n",
        "        # will be padded, we tell Keras to mask everywhere it finds a zero\n",
        "        embedding = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                           output_dim=self.embedding_dim,\n",
        "                                           mask_zero=True)\n",
        "        embedded_text = embedding(text)\n",
        "\n",
        "        # An RNN needs to take a Cell as an input. This is the diagram\n",
        "        # from lecture of the LSTM.\n",
        "        lstm = keras.layers.RNN(\n",
        "            cell=keras.layers.LSTMCell(units=self.lstm_size)\n",
        "        )\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # We can process both directions\n",
        "            lstm = keras.layers.Bidirectional(lstm)\n",
        "\n",
        "        # Keras will, by default, return us the state after processing the\n",
        "        # entire document (not the state for evert element in the input seq.)\n",
        "        h = lstm(embedded_text)\n",
        "\n",
        "        # We will use dropout to regularize our model while passing it through\n",
        "        # a single fully connected layer.\n",
        "        h = keras.layers.Dropout(0.5)(h)\n",
        "        h = keras.layers.Dense(self.lstm_size // 2, activation='relu')(h)\n",
        "        h = keras.layers.Dropout(0.5)(h)\n",
        "\n",
        "        # This is a binary problem, so we have a single output\n",
        "        output = keras.layers.Dense(1, activation='sigmoid')(h)\n",
        "\n",
        "      \n",
        "        model = keras.Model(text, output)\n",
        "        if shouldVisualize:\n",
        "            # print(\"this is our model summary\")\n",
        "            # print(model.summary())\n",
        "            keras.utils.plot_model(model, \"bc_withshapes.png\", show_shapes=True)\n",
        "\n",
        "        # We're going to use crossentropy as our loss function here\n",
        "        model.compile(optimizer=self.optimizer,\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, documents, labels, validation_data=None, pad_length='max',\n",
        "            **kwargs):\n",
        "        \"\"\"Trains (or fits) the model on training data while validating\n",
        "        on validation data\n",
        "\n",
        "        Args:\n",
        "            documents (List[str]): A list of training documents\n",
        "            labels (List[str | int]): A list of training labels associated to\n",
        "                each document.\n",
        "            validation_data (Tuple[List[str], List[str]]): A tuple of\n",
        "                validation data of the form (val_documents, val_labels)\n",
        "            pad_length (str | int): The padding length to use for training.\n",
        "            **kwargs: Passed to keras.Model.fit\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If validation data is not of the correct format.\n",
        "        \"\"\"\n",
        "        #In order for us to call .fit, we need to define both an X and Y, \n",
        "        #where X is input data and y is our target data.\n",
        "\n",
        "        # X will be an array of integers corresponding to words\n",
        "        X = np.array(\n",
        "            self.vocab.fit(documents).transform(\n",
        "                documents, pad_length=pad_length)\n",
        "        )\n",
        "\n",
        "        # y will be an array of integers corresponding to sentiment labels (0\n",
        "        # or 1)\n",
        "        y = self.labelencoder.fit_transform(labels)\n",
        "\n",
        "        if validation_data:\n",
        "            # Process validation data the same way we process our training data\n",
        "            if not len(validation_data) == 2:\n",
        "                raise ValueError('Validation data must be a tuple (X, y)')\n",
        "            documents_val, labels_val = validation_data\n",
        "            validation_data = (\n",
        "                np.array(self.vocab.transform(\n",
        "                    documents_val, pad_length=pad_length)),\n",
        "                self.labelencoder.transform(labels_val)\n",
        "            )\n",
        "            _ = kwargs.pop('validation_data', None)\n",
        "\n",
        "        # Construct our Keras model\n",
        "        self.model = self.make_model(vocab_size=self.vocab.vocab_size)\n",
        "\n",
        "        # In practice, we would set up a *callback* in order to stop\n",
        "        # training when the validation error is minimized\n",
        "        self.model.fit(X, y, validation_data=validation_data, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, documents, pad_length='max', **kwargs):\n",
        "        # To run a prediction, we have to run all the way from:\n",
        "        # text -> tokenization -> integers -> model\n",
        "        # Here, we only care to get the probabilities for each class\n",
        "        indices = np.array(self.vocab.transform(\n",
        "            documents, pad_length=pad_length))\n",
        "        return self.model.predict(indices, **kwargs).ravel()\n",
        "\n",
        "    def predict(self, documents, pad_length='max', **kwargs):\n",
        "        # We can use the above method to get probabilities per class, then\n",
        "        # we just take the most likely one and recover the label\n",
        "        label_inv = self.predict_proba(documents, pad_length=pad_length,\n",
        "                                       **kwargs)\n",
        "        return self.labelencoder.inverse_transform(1 * (label_inv > 0.5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6go6u2x_me2s"
      },
      "source": [
        "sentiment_model = BinarySentimentModel(bidirectional=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rLzRZGdUkci"
      },
      "source": [
        "Here, we pass the param shouldVisualize into make_model in order to 1. Display a summary of the model layers and 2. Download a png of the model graph with keras.utils.plot_model. Note that this implementation is for instructional purposeses of visualizing our model, and it's advisable to remove code like this before production. However, .summary() remains a powerful tool for debugging models in development."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR0Y8GwvSLas",
        "outputId": "27ff2b62-7586-4b33-c7ae-25f9696eb9bd"
      },
      "source": [
        "sentiment_model.make_model(60,shouldVisualize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7f759d9a9510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtfBVqIlAfvK"
      },
      "source": [
        "text, labels = sentiment_data['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geBntsqVAheQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58962715-e75b-4af3-f1fd-8c361b2aa728"
      },
      "source": [
        "text[:5], labels[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['there is simply no excuse for something this poorly done.',\n",
              "  'they were excellent.',\n",
              "  'i have 2-3 bars on my cell phone when i am home, but you cant not hear anything.',\n",
              "  'there is nothing authentic about this place.',\n",
              "  'good prices.'],\n",
              " ['negative', 'positive', 'negative', 'negative', 'positive'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAh0dvT2me2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c001d14-cd44-47a0-d49d-556df747a49f"
      },
      "source": [
        "# In a real application, we would use a EarlyStopping and ModelCheckpoint \n",
        "# callback to stop training at the bottom of the validation loss curve.\n",
        "sentiment_model.fit(\n",
        "    *sentiment_data['train'],               # unpacks the tuple to docs and labels\n",
        "    validation_data=sentiment_data['val'], \n",
        "    epochs=3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "66/66 [==============================] - 34s 432ms/step - loss: 0.6511 - acc: 0.6190 - val_loss: 0.6572 - val_acc: 0.7650\n",
            "Epoch 2/3\n",
            "66/66 [==============================] - 29s 442ms/step - loss: 0.3298 - acc: 0.8776 - val_loss: 0.3630 - val_acc: 0.8500\n",
            "Epoch 3/3\n",
            "66/66 [==============================] - 30s 449ms/step - loss: 0.1742 - acc: 0.9533 - val_loss: 0.4283 - val_acc: 0.8283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BinarySentimentModel at 0x7f71f05f3a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vLeQEENme2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3e47ba-406c-4d3f-d03e-c46181d05a37"
      },
      "source": [
        "sentiment_model.predict([\n",
        "    'this was the worst product ive ever used!!!', \n",
        "    'pretty awesome product!!!'\n",
        "]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['negative', 'positive'], dtype='<U8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh0rIZzxme20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbc6b18-8ccd-4f91-a1fe-bb635bb8020a"
      },
      "source": [
        "sentiment_model.predict_proba([\n",
        "    'this was the worst product ive ever used!!!', \n",
        "    'this was the best product ive ever used!!!'\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.12760785, 0.9930373 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7nNUHWF0cA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efhrOTkcme22"
      },
      "source": [
        "# Virtual Assistant Dataset\n",
        "\n",
        "In this example, we're going to train a model that is able to peform query understanding on a dataset of weather requests to a virtual assistant. We're going to build an LSTM model to predict what recognized component of a query each word in a piece of dialog corresponds to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHbgsH9Gme22"
      },
      "source": [
        "# Let's get an example to understand\n",
        "example = assistant_data['train'][60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAed5m6Ime24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d829a3-acbc-413f-9b45-14d3777a5674"
      },
      "source": [
        "print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': 'Where is '}, {'text': 'Bernardsville', 'label': 'city'}, {'text': '?'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHg_Njyme26"
      },
      "source": [
        "How can entities be encoded into labels? There is much debate on this, but there are generally three approaches:\n",
        "\n",
        "* **IO-encoding**: Only encodes that a token is an entity of a given type or not\n",
        "* **BIO-encoding**: Encodes the begining of an entity with a `B`, and also any following tokens in an entity with an `I`\n",
        "* **BILUO-encoding**: Encodes the begining of an entity with a `B`, any following tokens in an entity with an `I`, and the last token of an entity with a `L`. Single token entities are a `U`\n",
        "\n",
        "For example:\n",
        "\n",
        "*How cold is it tomorrow evening?*\n",
        "\n",
        "IO:\n",
        "*How cold [`TEMP`] is it tomorrow [`TIME`] evening [`TIME`]?*\n",
        "\n",
        "BIO:\n",
        "*How cold [`B-TEMP`] is it tomorrow [`B-TIME`] evening [`I-TIME`]?*\n",
        "\n",
        "BILUO:\n",
        "*How cold [`U-TEMP`] is it tomorrow [`B-TIME`] evening [`L-TIME`]?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALEUL7yJme27"
      },
      "source": [
        "def get_word_labels(example, tokenizer=tokenize):\n",
        "    \"\"\"\n",
        "    We define a function that can take this data, and \n",
        "    return a sequence of tokens and a sequence of token-labels.\n",
        "    \"\"\"\n",
        "    tokenized_text = []\n",
        "    labels = []\n",
        "    for chunk in example:\n",
        "        tokenized_chunk_text = tokenize(chunk['text'])\n",
        "        if 'label' in chunk:\n",
        "            # We're doing a subobtimal thing jere\n",
        "            chunk_labels = [chunk['label'].upper()] * len(tokenized_chunk_text)\n",
        "        else:\n",
        "            chunk_labels = ['-'] * len(tokenized_chunk_text)\n",
        "        tokenized_text.extend(tokenized_chunk_text)\n",
        "        labels.extend(chunk_labels)\n",
        "    return tokenized_text, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftHkcuzVCmDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696ec79e-62ce-486c-d8f4-6ad0ea8dec38"
      },
      "source": [
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Where is '},\n",
              " {'label': 'city', 'text': 'Bernardsville'},\n",
              " {'text': '?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOPLeP2zme29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3938556-0e08-4f3e-dc3c-d2cb1db45cd7"
      },
      "source": [
        "get_word_labels(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Where', 'is', 'Bernardsville', '?'], ['-', '-', 'CITY', '-'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-l7yjb6me2-"
      },
      "source": [
        "\n",
        "class WeatherAssistantModel():\n",
        "\n",
        "    def __init__(self, embedding_dim=128, lstm_size=256, optimizer='adam'):\n",
        "        \"\"\"Create a new model to handle a weather assistant use case.\n",
        "\n",
        "        The wrapper class will hold all state with respect to preprocessing,\n",
        "        transformation, and model training.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the \"word vectors\" to be\n",
        "                trained in the model.\n",
        "            lstm_size (int): The number of hidden units in the LSTM.\n",
        "            optimizer (str | keras.optimizers.Optimizer): The optimizer to use\n",
        "                when optimizing the loss on the training set using Stochastic\n",
        "                Gradient Descent (SGD).\n",
        "        \"\"\"\n",
        "        # We encode the label space and the vocabulary as `VocabularyContainer`\n",
        "        # for padding. Our text is pretokenized, so we don't need a tokenizer\n",
        "        self.labelencoder = icmenlp.VocabularyContainer(tokenizer=lambda x: x)\n",
        "        self.vocab = icmenlp.VocabularyContainer(tokenizer=lambda x: x)\n",
        "        self.model = None\n",
        "\n",
        "        # Store our hyperparameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_size = lstm_size\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def make_model(self, nb_classes, vocab_size):\n",
        "        \"\"\"Creates a new keras model for the class.\n",
        "\n",
        "        Args:\n",
        "            nb_classes (int): The total number of entity classes to predict\n",
        "            vocab_size (int): The number of unique tokens contained in the\n",
        "                training vocabulary\n",
        "\n",
        "        Returns:\n",
        "            keras.models.Model: A built and compiled Keras model for the task.\n",
        "        \"\"\"\n",
        "\n",
        "        # The None tells Keras that we can expect differing sentence lengths\n",
        "        text = keras.Input(shape=(None, ), dtype='int32')\n",
        "        \n",
        "        # will be padded, we tell Keras to mask all tensor values equal to zero\n",
        "        embedding = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                           output_dim=self.embedding_dim,\n",
        "                                           mask_zero=True)\n",
        "        embedded_text = embedding(text)\n",
        "\n",
        "        # We use a bi-LSTM, but this time, we return the sequences per-token\n",
        "        # with the return_sequences argument (one value of the hidden state per\n",
        "        # input token)\n",
        "        bilstm = keras.layers.Bidirectional(\n",
        "            keras.layers.RNN(\n",
        "                keras.layers.LSTMCell(self.lstm_size),\n",
        "                return_sequences=True\n",
        "            )\n",
        "        )\n",
        "        h = bilstm(embedded_text)\n",
        "\n",
        "        # Output the distribution of entites per timestep\n",
        "        output = keras.layers.Dense(nb_classes, activation='softmax')(h)\n",
        "\n",
        "        model = keras.Model(text, output)\n",
        "        model.compile(optimizer=self.optimizer,\n",
        "                      loss='sparse_categorical_crossentropy')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, chunked_documents, validation_data=None, **kwargs):\n",
        "        \"\"\"Trains (or fits) the model on training data while validating\n",
        "        on validation data\n",
        "\n",
        "        Args:\n",
        "            chunked_documents (List[List[Dict]]): Training data\n",
        "            validation_data (List[List[Dict]]): Validation data\n",
        "            **kwargs: Passed to keras.Model.fit\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Get paired pre-tokenized documents with per-token tags\n",
        "        documents, labels = zip(*[\n",
        "            get_word_labels(example, tokenizer=tokenize)\n",
        "            for example in chunked_documents\n",
        "        ])\n",
        "\n",
        "        # We want our input documents and our output labels to be integers\n",
        "        X = np.array(self.vocab.fit(documents).transform(\n",
        "            documents, pad_length=30))\n",
        "        Y = np.expand_dims(\n",
        "            np.array(self.labelencoder.fit(\n",
        "                labels).transform(labels, pad_length=30)),\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        nb_classes = self.labelencoder.vocab_size\n",
        "\n",
        "        if validation_data:\n",
        "            # Process validation data identically to training data\n",
        "            documents_val, labels_val = zip(*[\n",
        "                get_word_labels(example, tokenizer=tokenize)\n",
        "                for example in validation_data\n",
        "            ])\n",
        "            validation_data = (\n",
        "                np.array(self.vocab.transform(documents_val, pad_length=30)),\n",
        "                np.expand_dims(np.array(self.labelencoder.transform(\n",
        "                    labels_val, pad_length=30)), -1)\n",
        "            )\n",
        "            _ = kwargs.pop('validation_data', None)\n",
        "\n",
        "        self.model = self.make_model(nb_classes=nb_classes,\n",
        "                                     vocab_size=self.vocab.vocab_size)\n",
        "\n",
        "        # In practice, we would set up a callback in order to stop\n",
        "        # training when the validation error is minimized\n",
        "        self.model.fit(X, Y, validation_data=validation_data, **kwargs)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, document):\n",
        "        # To run inference on a single document, we tokenize it, obtain indices\n",
        "        # run it though the model, then decode the most likely tags per token.\n",
        "        segments, _ = get_word_labels([{'text': document}], tokenizer=tokenize)\n",
        "        indices = np.array(self.vocab.transform(segments))\n",
        "        label_inv = self.model.predict(indices).argmax(-1).astype(int).tolist()\n",
        "        return list(zip(\n",
        "            segments, self.labelencoder.inverse_transform(label_inv)\n",
        "        ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ViNITnme3A"
      },
      "source": [
        "assistant_model = WeatherAssistantModel(lstm_size=256, embedding_dim=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ-WzXXfme3E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "cd09ba80-9268-4aab-9aca-379f6d900ec3"
      },
      "source": [
        "# In a real application, we would use a EarlyStopping and ModelCheckpoint \n",
        "# callback to stop training at the bottom of the validation loss curve.\n",
        "assistant_model.fit(assistant_data['train'], assistant_data['val'], \n",
        "                    epochs=20, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "44/44 [==============================] - 7s 157ms/step - loss: 0.5884 - val_loss: 0.4196\n",
            "Epoch 2/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.3960 - val_loss: 0.3291\n",
            "Epoch 3/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.2910 - val_loss: 0.2292\n",
            "Epoch 4/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.2000 - val_loss: 0.1577\n",
            "Epoch 5/20\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 0.1208 - val_loss: 0.1066\n",
            "Epoch 6/20\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 0.0765 - val_loss: 0.0844\n",
            "Epoch 7/20\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 0.0539 - val_loss: 0.0730\n",
            "Epoch 8/20\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 0.0372 - val_loss: 0.0726\n",
            "Epoch 9/20\n",
            "44/44 [==============================] - 7s 150ms/step - loss: 0.0240 - val_loss: 0.0621\n",
            "Epoch 10/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.0147 - val_loss: 0.0573\n",
            "Epoch 11/20\n",
            "44/44 [==============================] - 6s 147ms/step - loss: 0.0097 - val_loss: 0.0564\n",
            "Epoch 12/20\n",
            "44/44 [==============================] - 6s 147ms/step - loss: 0.0072 - val_loss: 0.0594\n",
            "Epoch 13/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.0052 - val_loss: 0.0549\n",
            "Epoch 14/20\n",
            "44/44 [==============================] - 7s 149ms/step - loss: 0.0041 - val_loss: 0.0550\n",
            "Epoch 15/20\n",
            "44/44 [==============================] - 6s 147ms/step - loss: 0.0032 - val_loss: 0.0574\n",
            "Epoch 16/20\n",
            "44/44 [==============================] - 6s 141ms/step - loss: 0.0027 - val_loss: 0.0588\n",
            "Epoch 17/20\n",
            "44/44 [==============================] - 6s 145ms/step - loss: 0.0021 - val_loss: 0.0579\n",
            "Epoch 18/20\n",
            "44/44 [==============================] - 7s 151ms/step - loss: 0.0017 - val_loss: 0.0560\n",
            "Epoch 19/20\n",
            "44/44 [==============================] - 7s 151ms/step - loss: 0.0015 - val_loss: 0.0597\n",
            "Epoch 20/20\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 0.0013 - val_loss: 0.0578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.WeatherAssistantModel at 0x7fbd3791abe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFh21quRme3F"
      },
      "source": [
        "result = assistant_model.predict(\"How cold is it tomorrow?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkzYYs_4me3H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1d0f92e5-e14b-40f3-84f0-2c6e70c153c3"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('How', ['-']),\n",
              " ('cold', ['CONDITION_TEMPERATURE']),\n",
              " ('is', ['-']),\n",
              " ('it', ['-']),\n",
              " ('tomorrow', ['TIMERANGE']),\n",
              " ('?', ['-'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQD2uCScme3I"
      },
      "source": [
        "understood_entities = [\n",
        "    (word, ent_type[0])\n",
        "    for word, ent_type in result\n",
        "    if ent_type[0] != '-'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSCCPZWPme3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3229d05-11a4-4c21-a0e3-43b7be67c6bc"
      },
      "source": [
        "understood_entities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cold', 'CONDITION_TEMPERATURE'), ('tomorrow', 'TIMERANGE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AY0KoNrG-Re"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}